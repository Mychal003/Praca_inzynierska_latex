Oprócz metryk automatycznych (ROUGE, Semantic Similarity), przeprowadzono 
ewaluację z wykorzystaniem modelu językowego jako sędziego (LLM-as-a-Judge). 
Metoda ta pozwala na ocenę jakości odpowiedzi w wymiarach trudnych do 
uchwycenia przez metryki oparte na dopasowaniu tekstowym.

\subsection{Metodologia LLM Judge}

Do ewaluacji wykorzystano model GPT-4o, który oceniał każdą odpowiedź 
w pięciu wymiarach w skali 0.0--1.0:

\begin{itemize}
    \item \textbf{Correctness} -- poprawność merytoryczna odpowiedzi
    \item \textbf{Completeness} -- kompletność informacji
    \item \textbf{Relevance} -- adekwatność do pytania
    \item \textbf{Groundedness} -- oparcie odpowiedzi na dostarczonym kontekście
    \item \textbf{Overall} -- ogólna jakość odpowiedzi
\end{itemize}

\subsection{Porównanie konfiguracji}

W celu zbadania wpływu parametrów RAG na jakość odpowiedzi ocenianą 
przez LLM Judge, przeprowadzono ewaluację dla trzech konfiguracji:

\begin{itemize}
    \item \textbf{800/10/0} -- chunk\_size=800, k=10, overlap=0
    \item \textbf{800/10/100} -- chunk\_size=800, k=10, overlap=100
    \item \textbf{1200/10/0} -- chunk\_size=1200, k=10, overlap=0
\end{itemize}

Tabela~\ref{tab:llm_judge_comparison} przedstawia porównanie wyników 
dla wszystkich trzech konfiguracji.

\begin{table}[H]
\centering
\caption{Porównanie wyników LLM Judge dla różnych konfiguracji}
\label{tab:llm_judge_comparison}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metryka} & \textbf{800/10/0} & \textbf{800/10/100} & \textbf{1200/10/0} \\
\hline
ROUGE-1 F1 & \textbf{0.506} & 0.503 & 0.470 \\
Semantic Similarity & 0.774 & \textbf{0.775} & 0.752 \\
Latencja (s) & 3.62 & \textbf{3.20} & 5.30 \\
\hline
LLM Correctness & \textbf{0.964} & 0.960 & 0.956 \\
LLM Completeness & 0.918 & \textbf{0.924} & 0.882 \\
LLM Relevance & 0.918 & \textbf{0.928} & 0.914 \\
LLM Groundedness & 0.920 & \textbf{1.000} & 0.980 \\
LLM Overall & \textbf{0.940} & 0.938 & 0.930 \\
\hline
\end{tabular}
\end{table}

Rysunek~\ref{fig:llm_judge_comparison} przedstawia wizualizualne porównanie wymiarów LLM Judge dla wszystkich konfiguracji.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/results/llm_judge_comparison.png}
\caption{Porównanie wymiarów LLM Judge dla różnych konfiguracji}
\label{fig:llm_judge_comparison}
\end{figure}

\subsection{Analiza wyników}

Kluczową obserwacją wynikającą z przeprowadzonej ewaluacji jest 
\textbf{minimalna różnica między konfiguracjami}. Metryka LLM Overall 
waha się w zakresie 0.930--0.940, co stanowi różnicę zaledwie 1\%. 
Wszystkie konfiguracje osiągają bardzo wysokie wyniki, przekraczające 
93\% w ogólnej ocenie jakości.

\subsubsection{Wpływ parametru overlap na Groundedness}

Szczególnie interesującym wynikiem jest \textbf{perfekcyjny wynik 
Groundedness (1.000)} dla konfiguracji 800/10/100. Nakładanie się 
fragmentów (overlap=100) zapewnia ciągłość kontekstu między sąsiednimi 
fragmentami, co ułatwia modelowi powiązanie generowanych odpowiedzi 
z konkretnym miejscem w dokumencie.

Dla porównania:
\begin{itemize}
    \item overlap=100: Groundedness = 1.000
    \item overlap=0: Groundedness = 0.920
    \item chunk\_size=1200 (większe fragmenty): Groundedness = 0.980
\end{itemize}

Wynik ten pokazuje, że jeśli priorytetem jest minimalizacja halucynacji 
i maksymalne ugruntowanie odpowiedzi w źródle, warto rozważyć użycie 
parametru overlap.

\subsubsection{Wpływ rozmiaru fragmentów}

Konfiguracja z większymi fragmentami (1200/10/0) osiąga najniższe 
wyniki w większości metryk, przy jednoczesnym znacznym wzroście 
latencji (+46\% w porównaniu do 800/10/0). Większe fragmenty:

\begin{itemize}
    \item Nie poprawiają znacząco jakości odpowiedzi (LLM Overall: 0.930 vs 0.940)
    \item Zwiększają czas przetwarzania (5.30s vs 3.62s)
    \item Mogą wprowadzać szum informacyjny do kontekstu
\end{itemize}

\subsection{Wyniki według kategorii pytań}

Tabela~\ref{tab:category_llm_comparison} przedstawia porównanie 
wyników według kategorii pytań dla wszystkich konfiguracji.

\begin{table}[H]
\centering
\caption{Porównanie metryk według kategorii pytań}
\label{tab:category_llm_comparison}
\begin{tabular}{|l|l|c|c|c|}
\hline
\textbf{Kategoria} & \textbf{Metryka} & \textbf{800/10/0} & \textbf{800/10/100} & \textbf{1200/10/0} \\
\hline
Factual & ROUGE-1 & 0.432 & \textbf{0.453} & 0.449 \\
 & Semantic & 0.703 & \textbf{0.704} & \textbf{0.704} \\
\hline
Procedural & ROUGE-1 & \textbf{0.593} & 0.567 & 0.513 \\
 & Semantic & \textbf{0.834} & 0.829 & 0.791 \\
\hline
Troubleshooting & ROUGE-1 & 0.348 & \textbf{0.371} & 0.346 \\
 & Semantic & 0.723 & \textbf{0.753} & 0.729 \\
\hline
\end{tabular}
\end{table}

Rysunek~\ref{fig:llm_category_comparison} wizualizuje różnice 
w wynikach dla poszczególnych kategorii pytań.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/results/llm_category_comparison.png}
\caption{Porównanie metryk według kategorii pytań dla różnych konfiguracji}
\label{fig:llm_category_comparison}
\end{figure}

Analiza wyników według kategorii pokazuje, że:

\begin{itemize}
    \item \textbf{Pytania proceduralne} --- konfiguracja 800/10/0 
    osiąga najlepsze wyniki. Mniejsze fragmenty bez nakładania lepiej 
    izolują poszczególne kroki procedur.
    
    \item \textbf{Pytania faktograficzne} --- wszystkie konfiguracje 
    osiągają zbliżone wyniki, z minimalną przewagą konfiguracji z overlap.
    
    \item \textbf{Pytania diagnostyczne} --- konfiguracja 800/10/100 
    osiąga najlepsze wyniki. Nakładanie się fragmentów pomaga w połączeniu informacji z różnych części dokumentu.
\end{itemize}

\subsection{Wpływ modelu językowego na wyniki}

Najważniejszą obserwacją wynikającą z przeprowadzonych eksperymentów 
jest \textbf{minimalna różnica w wynikach między konfiguracjami}. 
Różnice w metryce LLM Overall nie przekraczają 1\%, podczas gdy 
wszystkie konfiguracje osiągają wyniki powyżej 93\%.

Zjawisko to można wyjaśnić wysoką jakością modelu GPT-4o, który 
skutecznie kompensuje różnice w jakości kontekstu. Model potrafi:

\begin{itemize}
    \item Wyciągać istotne informacje nawet z nieoptymalnie dobranych 
    fragmentów
    \item Ignorować szum informacyjny w kontekście
    \item Syntetyzować informacje z wielu fragmentów
\end{itemize}

Obserwacja ta ma istotne wnioski praktyczne -- przy użyciu 
zaawansowanych modeli językowych, precyzyjna optymalizacja parametrów 
RAG może być mniej krytyczna niż sugeruje literatura oparta na 
starszych modelach.

\subsection{Podsumowanie ewaluacji LLM Judge}

Ewaluacja z wykorzystaniem LLM Judge dla trzech konfiguracji pozwala 
sformułować następujące wnioski:

\begin{enumerate}
    \item \textbf{Różnice między konfiguracjami są minimalne} -- 
    LLM Overall waha się w zakresie 0.930--0.940 (różnica 1\%), 
    co potwierdza że GPT-4o skutecznie kompensuje różnice w parametrach RAG.
    
    \item \textbf{Wszystkie konfiguracje osiągają wysoką jakość} -- 
    Correctness $\geq$ 0.956, Overall $\geq$ 0.930 dla wszystkich 
    testowanych wariantów.
    
    \item \textbf{Overlap poprawia Groundedness} -- konfiguracja 
    z overlap=100 osiąga wynik 1.000, co sugeruje że 
    nakładanie się fragmentów minimalizuje ryzyko halucynacji.
    
    \item \textbf{Większe fragmenty nie poprawiają jakości} -- 
    chunk\_size=1200 zwiększa latencję o 46\% bez poprawy LLM Overall.
    
    \item \textbf{Wybór modelu językowego jest kluczowy} -- przy 
    użyciu GPT-4o optymalizacja parametrów RAG ma minimalny wpływ 
    na końcową jakość odpowiedzi.
\end{enumerate}