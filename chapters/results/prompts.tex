
Kluczowym wnioskiem wynikającym z przeprowadzonych analiz jest stwierdzenie, 
że jakość systemów RAG jest ściśle powiązana ze sposobem projektowania promptów. 
Innymi słowy, jakość generowanych odpowiedzi jest silnie uzależniona od precyzji 
i struktury zaprojektowanych instrukcji dla modelu językowego.

Przeprowadzono porównanie dwóch wersji promptów:

\begin{itemize}
    \item \textbf{Prompt restrykcyjny} --- instrukcja: \textit{,,jeśli nie znajdziesz 
    odpowiedzi w kontekście lub informacja jest niepełna, odpowiedz: nie mam tej informacji''}
    
    \item \textbf{Prompt elastyczny} --- instrukcja: \textit{,,odpowiedz na podstawie 
    kontekstu, jeśli kontekst nie zawiera pełnej informacji, podaj 
    co możesz i zaznacz braki''}
\end{itemize}

\subsection{Porównanie wyników}

Tabela~\ref{tab:prompt_comparison} przedstawia porównanie wyników 
dla obu wersji promptów przy identycznej konfiguracji parametrów 
(chunk\_size=800, k=10, overlap=0).

\begin{table}[h]
\centering
\caption{Porównanie wyników dla różnych wersji promptów}
\label{tab:prompt_comparison}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metryka} & \textbf{Prompt restrykcyjny} & \textbf{Prompt elastyczny} & \textbf{Zmiana} \\
\hline
ROUGE-1 F1 & 0.393 & 0.506 & +29\% \\
Semantic Similarity & 0.542 & 0.774 & +43\% \\
Success Rate & 68\% & 100\% & +32pp \\
\hline
LLM Correctness & 0.634 & 0.964 & +52\% \\
LLM Completeness & 0.578 & 0.918 & +59\% \\
LLM Relevance & 0.600 & 0.918 & +53\% \\
LLM Groundedness & 0.880 & 0.920 & +5\% \\
LLM Overall & 0.642 & 0.940 & +46\% \\
\hline
\end{tabular}
\end{table}

\subsection{Analiza wpływu promptów}

Zmiana promptu z restrykcyjnego na elastyczny przyniosła znaczącą poprawę 
we wszystkich metrykach:

\begin{itemize}
    \item \textbf{Success Rate wzrósł z 68\% do 100\%} -- system przestał 
    odmawiać odpowiedzi. Przy restrykcyjnym prompcie system odpowiadał 
    ,,nie mam tej informacji'' dla 8 z 25 pytań, mimo że kontekst 
    zawierał odpowiednie informacje. Prompt restrykcyjny powodował, że 
    model zbyt ostrożnie interpretował dostępność informacji.
    
    \item \textbf{LLM Correctness wzrósł z 63.4\% do 96.4\%} -- odpowiedzi 
    stały się znacznie bardziej poprawne merytorycznie. Model zaczął 
    wykorzystywać informacje z kontekstu zamiast odmawiać odpowiedzi, uzasadniając to brakiem informacji w kontekście.
    
    \item \textbf{LLM Completeness wzrósł z 57.8\% do 91.8\%} -- odpowiedzi 
    zawierają więcej kluczowych informacji. Elastyczny prompt zachęca 
    model do podawania wszystkich dostępnych informacji.
    
    \item \textbf{LLM Relevance wzrósł z 60.0\% do 91.8\%} -- odpowiedzi 
    są bardziej adekwatne do zadanych pytań.
    
    \item \textbf{LLM Groundedness wzrósł z 88.0\% do 92.0\%} -- wbrew 
    początkowym założeniom, elastyczny prompt nie obniżył jakości ugruntowania 
    odpowiedzi w kontekście. System nadal opiera odpowiedzi na dostarczonych 
    fragmentach dokumentu, jednocześnie lepiej wykorzystując dostępne informacje.
    
    \item \textbf{LLM Overall wzrósł z 64.2\% do 94.0\%} -- ogólna jakość 
    odpowiedzi poprawiła się o 46\%.
\end{itemize}

\subsection{Wnioski dotyczące projektowania promptów}

Wyniki potwierdzają, że projektowanie promptów (prompt engineering) ma 
znacznie większy wpływ na jakość systemu RAG niż optymalizacja parametrów 
technicznych:

\begin{itemize}
    \item Zmiana promptu dała poprawę \textbf{+46\%} w metryce LLM Overall
    \item Optymalizacja parametrów (chunk\_size, k, overlap) dała poprawę 
    rzędu \textbf{kilku procent}
\end{itemize}
